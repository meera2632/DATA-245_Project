{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"air quality data 2015 -2020.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")\n"
      ],
      "metadata": {
        "id": "5_ikrqj1r9zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_cities = df['City'].unique()\n",
        "print(\"\\nüèôÔ∏è Total Unique Cities:\", len(distinct_cities))\n",
        "print(\"Sample Cities:\", distinct_cities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ECeihtXvTA3",
        "outputId": "8871cc60-6baf-46ec-d35f-4ec5365c7098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèôÔ∏è Total Unique Cities: 22\n",
            "Sample Cities: ['ahmedabad' 'aizawl' 'amaravati' 'amritsar' 'bengaluru' 'bhopal'\n",
            " 'brajrajnagar' 'chandigarh' 'chennai' 'delhi' 'gurugram' 'guwahati'\n",
            " 'hyderabad' 'jaipur' 'jorapokhar' 'kolkata' 'lucknow' 'mumbai' 'patna'\n",
            " 'shillong' 'talcher' 'thiruvananthapuram']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"air quality data 2015 -2020.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Identify numeric columns EXCEPT 'AQI'\n",
        "numeric_cols = df.select_dtypes(include='number').columns.difference(['Year', 'Month', 'AQI'])\n",
        "\n",
        "# Impute using City-Year-Month group means (excluding AQI)\n",
        "df[numeric_cols] = df.groupby(['City', 'Year', 'Month'])[numeric_cols].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"air_quality_cleaned_seasonal.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Cleaned file saved as 'air_quality_cleaned_seasonal.csv' (AQI not imputed)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQCRwiMi5Pfm",
        "outputId": "7d5be4e0-d42c-4043-924e-338fc547e149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned file saved as 'air_quality_cleaned_seasonal.csv' (AQI not imputed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"/content/air_quality_cleaned_seasonal.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"cleaned_missing_values_by_city_year.csv\")\n"
      ],
      "metadata": {
        "id": "ELHPZ-tk6cW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"air_quality_cleaned_seasonal.csv\")\n",
        "\n",
        "# Define key pollutant columns\n",
        "pollutant_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO',\n",
        "                  'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene']\n",
        "\n",
        "# Drop rows where all pollutant columns are missing\n",
        "df_cleaned = df.dropna(subset=pollutant_cols, how='all')\n",
        "\n",
        "# Save cleaned version\n",
        "df_cleaned.to_csv(\"air_quality_final_clean.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Rows with all pollutants missing were dropped.\")\n",
        "print(\"‚úÖ Cleaned dataset saved as 'air_quality_final_clean.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEnUArooCJOL",
        "outputId": "9c176c7f-ed12-4d49-d432-971a8fde6a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Rows with all pollutants missing were dropped.\n",
            "‚úÖ Cleaned dataset saved as 'air_quality_final_clean.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"air_quality_cleaned_seasonal.csv\")\n",
        "\n",
        "# Drop rows where more than 8 columns are NaN\n",
        "df_cleaned = df[df.isnull().sum(axis=1) <= 8]\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df_cleaned.to_csv(\"air_quality_final_less_than_8_missing.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Dropped rows where more than 8 columns were missing.\")\n",
        "print(\"‚úÖ Saved as 'air_quality_final_less_than_8_missing.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYhumIH0CJ-k",
        "outputId": "7bc679b5-ef4a-4888-f806-6788da52bb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dropped rows where more than 8 columns were missing.\n",
            "‚úÖ Saved as 'air_quality_final_less_than_8_missing.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your cleaned file with less than 8 missing columns per row\n",
        "df = pd.read_csv(\"air_quality_final_less_than_8_missing.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Select numeric columns to impute, excluding AQI\n",
        "numeric_cols = df.select_dtypes(include='number').columns.difference(['Year', 'Month', 'AQI'])\n",
        "\n",
        "# Step 1: Impute using City-Year-Month group mean\n",
        "#df[numeric_cols] = df.groupby(['City', 'Year', 'Month'])[numeric_cols].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "# Step 2: Fallback imputation using City-Year mean\n",
        "df[numeric_cols] = df.groupby(['City', 'Year'])[numeric_cols].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "# # Final missing check\n",
        "# print(\"üîç Remaining missing values after both imputation steps:\\n\")\n",
        "# print(df[numeric_cols].isnull().sum())\n",
        "\n",
        "# Save final dataset\n",
        "df.to_csv(\"air_quality_final_fully_imputed.csv\", index=False)\n",
        "print(\"\\n‚úÖ Fully imputed dataset saved as 'air_quality_final_fully_imputed.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuWL-4FDEvgl",
        "outputId": "cc92ec69-e7c5-4663-bd75-a6f1816aa56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Fully imputed dataset saved as 'air_quality_final_fully_imputed.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"air_quality_final_fully_imputed.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")"
      ],
      "metadata": {
        "id": "LKMEew3IGI4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4IHmLummAaFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"airquality.csv\")\n",
        "distinct_cities = df['City'].unique()\n",
        "print(\"\\nüèôÔ∏è Total Unique Cities:\", len(distinct_cities))\n",
        "print(\"Sample Cities:\", distinct_cities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5fua-vWGsMs",
        "outputId": "9f79ade7-0c2d-45e3-af5d-00bd32488b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèôÔ∏è Total Unique Cities: 26\n",
            "Sample Cities: ['Aizawl' 'Gaziabad' 'Indore' 'Nagpur' 'Vishakhapatnam' 'Ahmedabad'\n",
            " 'Amaravati' 'Amritsar' 'Bengaluru' 'Bhopal' 'Brajrajnagar' 'Chandigarh'\n",
            " 'Chennai' 'Delhi' 'Gurugram' 'Guwahati' 'Hyderabad' 'Jaipur' 'Jorapokhar'\n",
            " 'Kolkata' 'Lucknow' 'Mumbai' 'Patna' 'Shillong' 'Talcher'\n",
            " 'Thiruvananthapuram']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")"
      ],
      "metadata": {
        "id": "wX_QQdfWMv_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"airquality.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Identify numeric columns EXCEPT 'AQI'\n",
        "numeric_cols = df.select_dtypes(include='number').columns.difference(['Year', 'Month', 'Computed_AQI'])\n",
        "\n",
        "# Impute using City-Year-Month group means (excluding AQI)\n",
        "df[numeric_cols] = df.groupby(['City', 'Year', 'Month'])[numeric_cols].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"air_quality_cleaned_seasonal.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Cleaned file saved as 'air_quality_cleaned_seasonal.csv' (AQI not imputed)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJTs3dpFM7_8",
        "outputId": "2c35cceb-728d-40df-f782-d3a7f00df3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned file saved as 'air_quality_cleaned_seasonal.csv' (AQI not imputed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1rG3mxTOAII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"air_quality_cleaned_seasonal.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")"
      ],
      "metadata": {
        "id": "FrfYXBdFOdQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"air_quality_cleaned_seasonal.csv\")\n",
        "\n",
        "# Ensure preprocessing columns\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Step 1: Create Season column\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'Winter'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'Summer'\n",
        "    elif month in [6, 7, 8, 9]:\n",
        "        return 'Monsoon'\n",
        "    else:\n",
        "        return 'Post-Monsoon'\n",
        "\n",
        "df['Season'] = df['Month'].apply(get_season)\n",
        "\n",
        "# Step 2: Define pollutant columns (excluding AQI if you're predicting it)\n",
        "pollutant_cols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'NH3']\n",
        "\n",
        "# Step 3: Impute missing values using City + Season group mean\n",
        "df[pollutant_cols] = df.groupby(['City', 'Year', 'Season'])[pollutant_cols].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "# Step 4: Check if any missing values still remain\n",
        "print(\"üîç Remaining missing values after seasonal imputation:\")\n",
        "print(df[pollutant_cols].isnull().sum())\n",
        "\n",
        "# Step 5: Save the final fully imputed dataset\n",
        "df.to_csv(\"air_quality_final_season_imputed.csv\", index=False)\n",
        "print(\"\\n‚úÖ Seasonally imputed dataset saved as 'air_quality_final_season_imputed.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS9g3g7COkf7",
        "outputId": "047839fd-e450-4e97-ad4a-1c7a9fb9af88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Remaining missing values after seasonal imputation:\n",
            "PM2.5    2283\n",
            "PM10     8616\n",
            "SO2      1446\n",
            "NO2      1536\n",
            "CO        549\n",
            "O3       1434\n",
            "NH3      8170\n",
            "dtype: int64\n",
            "\n",
            "‚úÖ Seasonally imputed dataset saved as 'air_quality_final_season_imputed.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"air_quality_final_season_imputed.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")"
      ],
      "metadata": {
        "id": "Fim1gG3kR7bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 1: Load your already cleaned and preprocessed file\n",
        "df = pd.read_csv(\"air_quality_final_season_imputed.csv\")\n",
        "\n",
        "# Step 2: Extract Date info (if not already done)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Step 3: Define features to use for predicting PM2.5 (change as needed)\n",
        "features = ['PM10', 'NO2', 'SO2', 'CO', 'NH3', 'O3', 'Month', 'Year']\n",
        "target = 'PM2.5'\n",
        "\n",
        "# Step 4: Split data into train (not null) and predict (null)\n",
        "df_train = df[df[target].notnull()]\n",
        "df_predict = df[df[target].isnull()]\n",
        "\n",
        "# Step 5: Prepare training and prediction matrices\n",
        "X_train = df_train[features]\n",
        "y_train = df_train[target]\n",
        "X_pred = df_predict[features]\n",
        "\n",
        "# Step 6: Train model and predict\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predicted_values = model.predict(X_pred)\n",
        "\n",
        "# Step 7: Fill predicted values back into the original DataFrame\n",
        "df.loc[df[target].isnull(), target] = predicted_values\n",
        "\n",
        "# Step 8: Save the updated dataset\n",
        "df.to_csv(\"air_quality_PM25_imputed_model.csv\", index=False)\n",
        "print(\"‚úÖ File saved as 'air_quality_PM25_imputed_model.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dllBCLz7VwXP",
        "outputId": "114eb95d-c08e-4c80-da5f-f5aed72f77d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ File saved as 'air_quality_PM25_imputed_model.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 1: Load your cleaned dataset\n",
        "df = pd.read_csv(\"air_quality_final_season_imputed.csv\")\n",
        "\n",
        "# Step 2: Preprocessing date\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Step 3: List of pollutant columns to impute using ML (excluding AQI)\n",
        "pollutants_to_impute = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'NH3']\n",
        "\n",
        "# Step 4: Define common features for all models\n",
        "base_features = ['Month', 'Year']\n",
        "other_pollutants = [col for col in pollutants_to_impute]  # will update dynamically\n",
        "\n",
        "# Step 5: Impute each pollutant one by one\n",
        "for target in pollutants_to_impute:\n",
        "    print(f\"üîß Imputing missing values for: {target}\")\n",
        "\n",
        "    # Features = all other pollutants + Month + Year\n",
        "    features = [col for col in pollutants_to_impute if col != target] + base_features\n",
        "\n",
        "    df_train = df[df[target].notnull()]\n",
        "    df_predict = df[df[target].isnull()]\n",
        "\n",
        "    # Check if there's data to train on\n",
        "    if not df_train.empty and not df_predict.empty:\n",
        "        X_train = df_train[features]\n",
        "        y_train = df_train[target]\n",
        "        X_pred = df_predict[features]\n",
        "\n",
        "        # Train model and predict\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_pred)\n",
        "\n",
        "        # Fill predictions into the original dataframe\n",
        "        df.loc[df[target].isnull(), target] = y_pred\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Skipping {target} due to lack of data.\")\n",
        "\n",
        "# Step 6: Save the final dataset\n",
        "df.to_csv(\"air_quality_all_pollutants_imputed.csv\", index=False)\n",
        "print(\"\\n‚úÖ Final file saved as 'air_quality_all_pollutants_imputed.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1Xyv6BdmZPRQ",
        "outputId": "bec4b3af-a9de-43b4-d96d-23b7b0674e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Imputing missing values for: PM2.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-01a1580cc87a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Train model and predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"air_quality_all_pollutants_imputed.csv\")  # use your working file\n",
        "\n",
        "# Ensure date format and extract year (if not already done)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "\n",
        "# List of pollutants\n",
        "pollutants = ['PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'O3', 'NH3']\n",
        "\n",
        "# Group by City and Year, then count missing values for each pollutant\n",
        "missing_by_city_year = df.groupby(['City', 'Year'])[pollutants].apply(lambda x: x.isnull().sum()).reset_index()\n",
        "\n",
        "# Display result\n",
        "print(\"üîç Missing values per City-Year for each pollutant:\")\n",
        "print(missing_by_city_year.head(20))  # show top 10 for preview\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd99daNYafl4",
        "outputId": "97f4a268-912f-4ca4-fe3a-238dfaf96bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Missing values per City-Year for each pollutant:\n",
            "         City  Year  PM2.5  PM10  NO2  SO2  CO  O3  NH3\n",
            "0   ahmedabad  2015      0     0    0    0   0   0    0\n",
            "1   ahmedabad  2016      0     0    0    0   0   0    0\n",
            "2   ahmedabad  2017      0     0    0    0   0   0    0\n",
            "3   ahmedabad  2018      0     0    0    0   0   0    0\n",
            "4   ahmedabad  2019      0     0    0    0   0   0    0\n",
            "5   ahmedabad  2020      0     0    0    0   0   0    0\n",
            "6      aizawl  2020      0     0    0    0   0   0    0\n",
            "7   amaravati  2017      0     0    0    0   0   0    0\n",
            "8   amaravati  2018      0     0    0    0   0   0    0\n",
            "9   amaravati  2019      0     0    0    0   0   0    0\n",
            "10  amaravati  2020      0     0    0    0   0   0    0\n",
            "11   amritsar  2017      0     0    0    0   0   0    0\n",
            "12   amritsar  2018      0     0    0    0   0   0    0\n",
            "13   amritsar  2019      0     0    0    0   0   0    0\n",
            "14   amritsar  2020      0     0    0    0   0   0    0\n",
            "15  bengaluru  2015      0     0    0    0   0   0    0\n",
            "16  bengaluru  2016      0     0    0    0   0   0    0\n",
            "17  bengaluru  2017      0     0    0    0   0   0    0\n",
            "18  bengaluru  2018      0     0    0    0   0   0    0\n",
            "19  bengaluru  2019      0     0    0    0   0   0    0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"air_quality_all_pollutants_imputed.csv\")\n",
        "\n",
        "# Define breakpoints for pollutants as per CPCB\n",
        "breakpoints = {\n",
        "    'PM2.5': [\n",
        "        (0, 30, 0, 50), (31, 60, 51, 100), (61, 90, 101, 200),\n",
        "        (91, 120, 201, 300), (121, 250, 301, 400), (251, 350, 401, 500)\n",
        "    ],\n",
        "    'PM10': [\n",
        "        (0, 50, 0, 50), (51, 100, 51, 100), (101, 250, 101, 200),\n",
        "        (251, 350, 201, 300), (351, 430, 301, 400), (431, 500, 401, 500)\n",
        "    ],\n",
        "    'NO2': [\n",
        "        (0, 40, 0, 50), (41, 80, 51, 100), (81, 180, 101, 200),\n",
        "        (181, 280, 201, 300), (281, 400, 301, 400), (401, 500, 401, 500)\n",
        "    ],\n",
        "    'SO2': [\n",
        "        (0, 40, 0, 50), (41, 80, 51, 100), (81, 380, 101, 200),\n",
        "        (381, 800, 201, 300), (801, 1600, 301, 400), (1601, 2000, 401, 500)\n",
        "    ],\n",
        "    'CO': [\n",
        "        (0.0, 1.0, 0, 50), (1.1, 2.0, 51, 100), (2.1, 10.0, 101, 200),\n",
        "        (10.1, 17.0, 201, 300), (17.1, 34.0, 301, 400), (34.1, 50.0, 401, 500)\n",
        "    ],\n",
        "    'O3': [\n",
        "        (0, 50, 0, 50), (51, 100, 51, 100), (101, 168, 101, 200),\n",
        "        (169, 208, 201, 300), (209, 748, 301, 400), (749, 1000, 401, 500)\n",
        "    ],\n",
        "    'NH3': [\n",
        "        (0, 200, 0, 50), (201, 400, 51, 100), (401, 800, 101, 200),\n",
        "        (801, 1200, 201, 300), (1201, 1800, 301, 400), (1801, 2400, 401, 500)\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Function to calculate sub-index for a pollutant\n",
        "def calculate_sub_index(value, pollutant):\n",
        "    for (bp_lo, bp_hi, i_lo, i_hi) in breakpoints.get(pollutant, []):\n",
        "        if bp_lo <= value <= bp_hi:\n",
        "            return ((i_hi - i_lo) / (bp_hi - bp_lo)) * (value - bp_lo) + i_lo\n",
        "    return None\n",
        "\n",
        "# Function to calculate AQI (max of all sub-indices)\n",
        "def calculate_aqi(row):\n",
        "    sub_indices = []\n",
        "    for pollutant in breakpoints:\n",
        "        value = row.get(pollutant)\n",
        "        if pd.notnull(value):\n",
        "            sub_index = calculate_sub_index(value, pollutant)\n",
        "            if sub_index is not None:\n",
        "                sub_indices.append(sub_index)\n",
        "    return max(sub_indices) if sub_indices else None\n",
        "\n",
        "# Apply AQI calculation to each row\n",
        "df['Computed_AQI'] = df.apply(calculate_aqi, axis=1)\n",
        "\n",
        "# Save final dataset\n",
        "df.to_csv(\"air_quality_with_computed_aqi.csv\", index=False)\n",
        "print(\"‚úÖ AQI computed and saved in 'air_quality_with_computed_aqi.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XymIwf_bagKE",
        "outputId": "30fb3b77-a16f-478f-e00d-034e4e3161c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AQI computed and saved in 'air_quality_with_computed_aqi.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset with Computed_AQI\n",
        "df = pd.read_csv(\"air_quality_with_computed_aqi.csv\")\n",
        "\n",
        "# Function to classify AQI into buckets\n",
        "def classify_aqi(aqi):\n",
        "    if pd.isnull(aqi):\n",
        "        return \"Unknown\"\n",
        "    elif aqi <= 50:\n",
        "        return \"Good\"\n",
        "    elif aqi <= 100:\n",
        "        return \"Satisfactory\"\n",
        "    elif aqi <= 200:\n",
        "        return \"Moderate\"\n",
        "    elif aqi <= 300:\n",
        "        return \"Poor\"\n",
        "    elif aqi <= 400:\n",
        "        return \"Very Poor\"\n",
        "    else:\n",
        "        return \"Severe\"\n",
        "\n",
        "# Apply classification\n",
        "df['AQI_Category'] = df['Computed_AQI'].apply(classify_aqi)\n",
        "\n",
        "# Save updated dataset\n",
        "df.to_csv(\"air_quality_with_aqi_category.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ AQI categories assigned and saved as 'air_quality_with_aqi_category.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG9RuHVmbyoH",
        "outputId": "ac255f9c-8ed8-454b-a1a1-321a7415a22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AQI categories assigned and saved as 'air_quality_with_aqi_category.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_excel(\"Combined_Weather_Data_2015_2020.xlsx\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")"
      ],
      "metadata": {
        "id": "Y8v61k7AcwJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_excel(\"Combined_Weather_Data_2015_2020.xlsx\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Identify numeric columns EXCEPT 'AQI'\n",
        "numeric_cols = df.select_dtypes(include='number').columns.difference(['Year', 'Month'])\n",
        "\n",
        "# Impute using City-Year-Month group means (excluding AQI)\n",
        "df[numeric_cols] = df.groupby(['City', 'Year', 'Month'])[numeric_cols].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"air_quality_cleaned_seasonal.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Cleaned file saved as 'weather_cleaned_seasonal.csv' (AQI not imputed)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8Cg8TserS1P",
        "outputId": "875e9ab5-272c-41d1-db0e-5eec97397566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned file saved as 'weather_cleaned_seasonal.csv' (AQI not imputed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_csv(\"air_quality_cleaned_seasonal.csv\")\n",
        "df['City'] = df['City'].str.strip().str.lower()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Group by City and Year, then count missing values (excluding City/Year)\n",
        "missing_by_city_year = df.drop(columns=['City', 'Year']).groupby([df['City'], df['Year']]).apply(lambda x: x.isnull().sum())\n",
        "\n",
        "# Add total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.sum(axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "missing_by_city_year.to_csv(\"missing_values_by_city_year.csv\")"
      ],
      "metadata": {
        "id": "IsUkHkzns7cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load weather dataset\n",
        "df = pd.read_csv(\"air_quality_cleaned_seasonal (1).csv\")\n",
        "\n",
        "# Convert Date and extract time features\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "# Add Season column\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'Winter'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'Summer'\n",
        "    elif month in [6, 7, 8, 9]:\n",
        "        return 'Monsoon'\n",
        "    else:\n",
        "        return 'Post-Monsoon'\n",
        "\n",
        "df['Season'] = df['Month'].apply(get_season)\n",
        "\n",
        "# Encode City and Season\n",
        "le_city = LabelEncoder()\n",
        "df['City_Code'] = le_city.fit_transform(df['City'])\n",
        "\n",
        "df['Season_Code'] = df['Season'].map({\n",
        "    'Winter': 0, 'Summer': 1, 'Monsoon': 2, 'Post-Monsoon': 3\n",
        "})\n",
        "\n",
        "# Columns to impute\n",
        "weather_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'Pressure9am', 'Temp9am']\n",
        "\n",
        "# Base features\n",
        "base_features = ['Month', 'Year', 'City_Code', 'Season_Code']\n",
        "all_features = weather_cols + base_features\n",
        "\n",
        "# Impute each weather column using other columns + encoded features\n",
        "for target in weather_cols:\n",
        "    print(f\"üîß Imputing missing values for: {target}\")\n",
        "\n",
        "    features = [col for col in all_features if col != target]\n",
        "    df_train = df[df[target].notnull()]\n",
        "    df_missing = df[df[target].isnull()]\n",
        "\n",
        "    if not df_train.empty and not df_missing.empty:\n",
        "        X_train = df_train[features]\n",
        "        y_train = df_train[target]\n",
        "        X_pred = df_missing[features]\n",
        "\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_pred)\n",
        "\n",
        "        df.loc[df[target].isnull(), target] = predictions\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Skipping {target} due to lack of missing or training data.\")\n",
        "\n",
        "# Save the fully imputed weather dataset\n",
        "df.to_csv(\"weather_rf_imputed_final.csv\", index=False)\n",
        "print(\"‚úÖ Weather data imputed and saved as 'weather_rf_imputed_final.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I6I7puAtcTH",
        "outputId": "36974f3e-61aa-4723-f505-fa5e94b9a2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Imputing missing values for: MinTemp\n",
            "‚ö†Ô∏è Skipping MinTemp due to lack of missing or training data.\n",
            "üîß Imputing missing values for: MaxTemp\n",
            "‚ö†Ô∏è Skipping MaxTemp due to lack of missing or training data.\n",
            "üîß Imputing missing values for: Rainfall\n",
            "üîß Imputing missing values for: WindGustSpeed\n",
            "üîß Imputing missing values for: Pressure9am\n",
            "üîß Imputing missing values for: Temp9am\n",
            "‚ö†Ô∏è Skipping Temp9am due to lack of missing or training data.\n",
            "‚úÖ Weather data imputed and saved as 'weather_rf_imputed_final.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load both cleaned datasets\n",
        "df_air = pd.read_csv(\"air_quality_final.csv\")\n",
        "df_weather = pd.read_csv(\"weather_final.csv\")\n",
        "\n",
        "# Clean and format Date and City\n",
        "df_air['Date'] = pd.to_datetime(df_air['Date'])\n",
        "df_weather['Date'] = pd.to_datetime(df_weather['Date'])\n",
        "\n",
        "df_air['City'] = df_air['City'].str.strip().str.lower()\n",
        "df_weather['City'] = df_weather['City'].str.strip().str.lower()\n",
        "\n",
        "# Filter air quality data for years 2017 to 2020\n",
        "df_air = df_air[df_air['Date'].dt.year.between(2017, 2020)]\n",
        "\n",
        "# Merge on City + Date\n",
        "merged_air_weather = pd.merge(df_air, df_weather, on=['City', 'Date'], how='inner')\n",
        "\n",
        "# Save the filtered merged dataset\n",
        "merged_air_weather.to_csv(\"merged_air_weather_2017_2020.csv\", index=False)\n",
        "print(\"‚úÖ Air + Weather merged for 2017‚Äì2020 and saved as 'merged_air_weather_2017_2020.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D__KQmkb1Te",
        "outputId": "0c3d1669-71c1-4d8c-ccfd-72d429828974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Air + Weather merged for 2017‚Äì2020 and saved as 'merged_air_weather_2017_2020.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"merged_air_weather_2017_2020.csv\")\n",
        "\n",
        "# Ensure Date is in datetime format and extract Year\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Calculate missing values per City-Year\n",
        "missing_by_city_year = (\n",
        "    df.groupby(['City', 'Year'], group_keys=False)\n",
        "      .apply(lambda x: x.isnull().sum(numeric_only=False))\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Add a total missing column\n",
        "missing_by_city_year['Total_Missing'] = missing_by_city_year.drop(columns=['City', 'Year']).sum(axis=1)\n",
        "\n",
        "# Show result\n",
        "print(missing_by_city_year.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "RGYiirKtdd8T",
        "outputId": "05037038-d3fc-4976-ae37-8a543070eac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-259e01657fbf>:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.isnull().sum(numeric_only=False))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot insert Year, already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-259e01657fbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6470\u001b[0m                     )\n\u001b[1;32m   6471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6472\u001b[0;31m                 new_obj.insert(\n\u001b[0m\u001b[1;32m   6473\u001b[0m                     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6474\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5157\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5160\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot insert Year, already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load daily air + weather data\n",
        "df_daily = pd.read_csv(\"merged_air_weather_2017_2020.csv\")\n",
        "df_traffic_monthly = pd.read_csv(\"synthetic_traffic_all_cities_2017_2020.csv\")\n",
        "\n",
        "# Step 1: Standardize City Names & Dates\n",
        "df_daily['City'] = df_daily['City'].str.strip().str.lower()\n",
        "df_traffic_monthly['City'] = df_traffic_monthly['City'].str.strip().str.lower()\n",
        "\n",
        "df_daily['Date'] = pd.to_datetime(df_daily['Date'])\n",
        "df_daily['Year'] = df_daily['Date'].dt.year\n",
        "df_daily['Month'] = df_daily['Date'].dt.month\n",
        "df_daily['DayOfWeek'] = df_daily['Date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
        "\n",
        "# Step 2: Merge monthly traffic data into daily records\n",
        "df_merged = pd.merge(\n",
        "    df_daily,\n",
        "    df_traffic_monthly[['City', 'Year', 'Month', 'Traffic_Congestion_Index']],\n",
        "    on=['City', 'Year', 'Month'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 3: Simulate realistic daily traffic using weekday/weekend pattern + noise\n",
        "def simulate_traffic(monthly_val, dow):\n",
        "    if pd.isna(monthly_val):\n",
        "        return np.nan\n",
        "    base = monthly_val * (0.85 if dow >= 5 else 1.0)  # Weekend traffic reduced\n",
        "    noise = np.random.normal(0, 0.03)  # Add ¬±3% random variation\n",
        "    return max(base + noise, 0)\n",
        "\n",
        "df_merged['Simulated_Traffic_Index'] = df_merged.apply(\n",
        "    lambda row: simulate_traffic(row['Traffic_Congestion_Index'], row['DayOfWeek']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Step 4: Save the result\n",
        "df_merged.to_csv(\"merged_air_weather_traffic_simulated_final.csv\", index=False)\n",
        "print(\"‚úÖ Final simulated dataset saved as 'merged_air_weather_traffic_simulated_final.csv'\")\n"
      ],
      "metadata": {
        "id": "6d_SywgRhksH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6accff-a32e-4caf-9bc0-4e5d74ff9633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final simulated dataset saved as 'merged_air_weather_traffic_simulated_final.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQRROpPG-l88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}